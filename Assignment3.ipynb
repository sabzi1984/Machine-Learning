{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment3.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "SxCSroV2xnmB",
        "bjggcDgTD7Yw",
        "HgKq5L2akSVP",
        "IoI6FR7she_N",
        "P5NSMoAiv4xl"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jeiv6GpRBWjP",
        "outputId": "a075b531-3847-49f3-a80e-593ef689dae9"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGW5yjvnDqQz"
      },
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import os\n",
        "import string"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0kDUH6SKCvy"
      },
      "source": [
        "PATH='/content/drive/MyDrive/Automn 2021/Machin Learning/Assignment3/medical_dataset'\n",
        "dataset=['train.csv','valid.csv','test.csv']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z60-V1JQR9kk"
      },
      "source": [
        "\n",
        "def lowercase(corpus):\n",
        "  corpus['text']=corpus['text'].str.lower()\n",
        "  return corpus\n",
        "\n",
        "def special_element_removal(corpus):\n",
        "  clean = re.compile('\\d\\.')\n",
        "  corpus[\"text\"] = corpus[\"text\"].apply(lambda x: clean.sub(' ',x)) #numbering of patient symptoms are removed\n",
        "  return corpus\n",
        "\n",
        "def tokenize(corpus):\n",
        "  clean = re.compile('[^a-zA-Z0-9]')\n",
        "  corpus[\"text\"] = corpus[\"text\"].apply(lambda x: clean.sub(' ',x))\n",
        "  clean = re.compile('[\\s\\t\\n]+')\n",
        "  corpus[\"words\"] = corpus[\"text\"].apply(lambda x: re.split('[\\s\\t\\n]+',x))\n",
        "  return corpus\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFasQfOEKt10"
      },
      "source": [
        "\n",
        "file_path=os.path.join(PATH, dataset[0])\n",
        "train_df=pd.read_csv(file_path)\n",
        "# print(f'{data.split(\".\")[0]}:\\n{df.head()}')\n",
        "train_df=lowercase(train_df)\n",
        "train_df=special_element_removal(train_df)\n",
        "train_df=tokenize(train_df)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6y7X4R7pzuc"
      },
      "source": [
        "file_path=os.path.join(PATH, dataset[1])\n",
        "valid_df=pd.read_csv(file_path)\n",
        "# print(f'{data.split(\".\")[0]}:\\n{df.head()}')\n",
        "valid_df=lowercase(valid_df)\n",
        "valid_df=special_element_removal(valid_df)\n",
        "valid_df=tokenize(valid_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yhi5v7xcp1k1"
      },
      "source": [
        "file_path=os.path.join(PATH, dataset[2])\n",
        "test_df=pd.read_csv(file_path)\n",
        "# print(f'{data.split(\".\")[0]}:\\n{df.head()}')\n",
        "test_df=lowercase(test_df)\n",
        "test_df=special_element_removal(test_df)\n",
        "test_df=tokenize(test_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fiywW0veh9S1"
      },
      "source": [
        "#N frequent word in corpus\n",
        "def frequent_words(corpus_df,N):\n",
        "  tokens=[]\n",
        "  for raw in corpus_df[\"words\"]:\n",
        "    for token in raw:\n",
        "      tokens.append(token)\n",
        "  bag_words={}\n",
        "  for token in tokens:\n",
        "    if token in bag_words:\n",
        "      bag_words[token]+=1\n",
        "    else:\n",
        "      bag_words[token]=1\n",
        "  df_freq=pd.DataFrame.from_dict(bag_words, orient='index',\n",
        "                       columns=['frequency'])\n",
        "  df_freq=df_freq.sort_values(by=['frequency'],ascending=False)\n",
        "  df_freq.index.names = ['Word']\n",
        "  df_freq=df_freq.reset_index()\n",
        "  # df_freq.index += 1 \n",
        "  df_freq['id']=df_freq.index+1 \n",
        "  df_freq = df_freq[['Word','id','frequency']]\n",
        "  return(df_freq.head(N))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDrdyrCKEaSI"
      },
      "source": [
        "freq_words_training=frequent_words(train_df,10000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apsdmuWc16wl"
      },
      "source": [
        "freq_words_training.to_csv(r'/content/drive/MyDrive/Automn 2021/Machin Learning/Assignment3/medical_nlp-vocab.txt', header=True, index=None, sep='\\t')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "id": "Z9yiS0K3W_6O",
        "outputId": "81f71568-d374-4236-81f5-6f50bb7fec01"
      },
      "source": [
        "freq_words_training=pd.read_csv('/content/drive/MyDrive/Automn 2021/Machin Learning/Assignment3/medical_nlp-vocab.txt', sep='\\t')\n",
        "freq_words_training"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Word</th>\n",
              "      <th>id</th>\n",
              "      <th>frequency</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>the</td>\n",
              "      <td>1</td>\n",
              "      <td>118887</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>and</td>\n",
              "      <td>2</td>\n",
              "      <td>66917</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>was</td>\n",
              "      <td>3</td>\n",
              "      <td>56124</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>of</td>\n",
              "      <td>4</td>\n",
              "      <td>48447</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>to</td>\n",
              "      <td>5</td>\n",
              "      <td>41003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9995</th>\n",
              "      <td>radiolucent</td>\n",
              "      <td>9996</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9996</th>\n",
              "      <td>l5s</td>\n",
              "      <td>9997</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9997</th>\n",
              "      <td>proteins</td>\n",
              "      <td>9998</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9998</th>\n",
              "      <td>equivalents</td>\n",
              "      <td>9999</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9999</th>\n",
              "      <td>suv</td>\n",
              "      <td>10000</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10000 rows Ã— 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             Word     id  frequency\n",
              "0             the      1     118887\n",
              "1             and      2      66917\n",
              "2             was      3      56124\n",
              "3              of      4      48447\n",
              "4              to      5      41003\n",
              "...           ...    ...        ...\n",
              "9995  radiolucent   9996          8\n",
              "9996          l5s   9997          8\n",
              "9997     proteins   9998          8\n",
              "9998  equivalents   9999          8\n",
              "9999          suv  10000          8\n",
              "\n",
              "[10000 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bxWtcixFhjF"
      },
      "source": [
        "# from sklearn.feature_extraction.text import CountVectorizer\n",
        "# train_df[\"joined_words\"] = train_df.words.apply(lambda x: \" \".join(x))\n",
        "# CountVec = CountVectorizer(vocabulary=freq_words_training['Word'][:40],binary=True,tokenizer=word_tokenize)\n",
        "# Count_data = CountVec.fit_transform(train_df[\"text\"])\n",
        "# feature_names = CountVec.get_feature_names()\n",
        "# binary_bgw_sk=pd.DataFrame.sparse.from_spmatrix(Count_data,columns=feature_names)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vG90unRdpFyI"
      },
      "source": [
        "#this method calculate binary bag of word \n",
        "import tqdm\n",
        "def binary_bgw(df,vocabulary):\n",
        "  binary_bgw=[]\n",
        "  nbr_word=len(vocabulary)\n",
        "  for row in tqdm.tqdm(df[\"words\"]):\n",
        "    bg=[0]*nbr_word\n",
        "    for i,word in enumerate(vocabulary):\n",
        "      if word in row:\n",
        "        bg[i]=1\n",
        "\n",
        "    binary_bgw.append(bg)\n",
        "  return binary_bgw\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCiCsYcS2XDQ",
        "outputId": "4f7492e3-4ed4-4c40-82aa-b030a39046b0"
      },
      "source": [
        "binary_bgw_train=binary_bgw(train_df,list(freq_words_training['Word']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4000/4000 [05:36<00:00, 11.87it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O89A5ezK2UcW"
      },
      "source": [
        "binary_bgw_train= pd.DataFrame(binary_bgw_train, columns=freq_words_training['Word'])\n",
        "binary_bgw_train.to_csv(r'/content/drive/MyDrive/Automn 2021/Machin Learning/Assignment3/binary-bgw_train.txt', header=True, index=None, sep='\\t')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjhxVju6_5Ow"
      },
      "source": [
        "binary_bgw_train=pd.read_csv('/content/drive/MyDrive/Automn 2021/Machin Learning/Assignment3/binary-bgw_train.txt', sep='\\t')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9N42fh_4CE9",
        "outputId": "e030d65b-158d-4d4f-c79b-bb621fd2cc53"
      },
      "source": [
        "binary_bgw_valid=binary_bgw(valid_df,list(freq_words_training['Word']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 499/499 [00:42<00:00, 11.82it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpIHuFZL4H8i"
      },
      "source": [
        "binary_bgw_valid= pd.DataFrame(binary_bgw_valid, columns=freq_words_training['Word'])\n",
        "binary_bgw_valid.to_csv(r'/content/drive/MyDrive/Automn 2021/Machin Learning/Assignment3/binary-bgw_valid.txt', header=True, index=None, sep='\\t')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-9ZDVOs4JrX"
      },
      "source": [
        "binary_bgw_valid=pd.read_csv('/content/drive/MyDrive/Automn 2021/Machin Learning/Assignment3/binary-bgw_valid.txt', sep='\\t')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-4OP53B4RAK",
        "outputId": "61914496-14af-4dad-b9ce-9596bc85a913"
      },
      "source": [
        "binary_bgw_test=binary_bgw(test_df,list(freq_words_training['Word']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:42<00:00, 11.67it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdQDWj5c4UBU"
      },
      "source": [
        "binary_bgw_test= pd.DataFrame(binary_bgw_test, columns=freq_words_training['Word'])\n",
        "binary_bgw_test.to_csv(r'/content/drive/MyDrive/Automn 2021/Machin Learning/Assignment3/binary-bgw_test.txt', header=True, index=None, sep='\\t')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9W8-n674WQo"
      },
      "source": [
        "binary_bgw_test=pd.read_csv('/content/drive/MyDrive/Automn 2021/Machin Learning/Assignment3/binary-bgw_test.txt', sep='\\t')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HyFDh3PV1UZb",
        "outputId": "e4570482-af93-43cc-fa78-65251cac0c26"
      },
      "source": [
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "binary_bgw_scrMatrix=csr_matrix(binary_bgw.values)\n",
        "binary_bgw_scrMatrix"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<4000x10000 sparse matrix of type '<class 'numpy.longlong'>'\n",
              "\twith 874180 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {},
          "execution_count": 178
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwRHEnacCWpY"
      },
      "source": [
        "import tqdm\n",
        "def freq_bgw(df,vocabulary):\n",
        "\n",
        "  freq_bgw=[]\n",
        "  # freq_word=list(freq_words_training['Word'])\n",
        "  nbr_word=len(vocabulary)\n",
        "  for row in tqdm.tqdm(df[\"words\"]):\n",
        "    bg=[0]*nbr_word\n",
        "    for word in row:\n",
        "      if word in vocabulary:\n",
        "        bg[vocabulary.index(word)]+=1\n",
        "    bg_fr=[x / sum(bg) for x in bg]\n",
        "    freq_bgw.append(bg_fr)\n",
        "  return freq_bgw"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1QYOUB0n2G07",
        "outputId": "4f8b73d8-d555-43ea-a97d-d1b5fafc1bad"
      },
      "source": [
        "#this method calculate frequency bag of word\n",
        "# import tqdm\n",
        "# freq_bgw=[]\n",
        "# freq_word=list(freq_words_training['Word'])\n",
        "# nbr_word=freq_words_training['Word'].shape[0]\n",
        "# for row in tqdm.tqdm(train_df.iloc[0:3,:][\"words\"]):\n",
        "#   bg=[0]*nbr_word\n",
        "#   for word in row:\n",
        "#     if word in freq_word:\n",
        "#       bg[freq_words_training[freq_words_training['Word']==word].index.values.astype(int)[0]]+=1\n",
        "#   bg_fr=[x / sum(bg) for x in bg]\n",
        "#   freq_bgw.append(bg_fr)\n",
        "\n",
        "# freq_bgw= pd.DataFrame(freq_bgw, columns=freq_words_training['Word'])\n",
        "# freq_bgw.to_csv(r'/content/drive/MyDrive/Automn 2021/Machin Learning/Assignment3/freq-bgw.txt', header=True, index=None, sep='\\t')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:03<00:00,  1.13s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pAuZ7ra7A_Bj",
        "outputId": "d97d1ecf-be23-4ca7-9347-c5190d2028e6"
      },
      "source": [
        "freq_bgw_train=freq_bgw(train_df,list(freq_words_training['Word']))\n",
        "freq_bgw_train= pd.DataFrame(freq_bgw_train, columns=freq_words_training['Word'])\n",
        "freq_bgw_train\n",
        "freq_bgw_train.to_csv(r'/content/drive/MyDrive/Automn 2021/Machin Learning/Assignment3/freq-bgw_train.txt', header=True, index=None, sep='\\t')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4000/4000 [55:20<00:00,  1.20it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "-Ub6egb_cfRv",
        "outputId": "d5ffeae8-7710-44a9-c5c0-a08d39666df7"
      },
      "source": [
        "freq_bgw_train=pd.read_csv(r'/content/drive/MyDrive/Automn 2021/Machin Learning/Assignment3/freq-bgw_train.txt', sep='\\t')\n",
        "freq_bgw_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>the</th>\n",
              "      <th>and</th>\n",
              "      <th>was</th>\n",
              "      <th>of</th>\n",
              "      <th>to</th>\n",
              "      <th>a</th>\n",
              "      <th>with</th>\n",
              "      <th>in</th>\n",
              "      <th>is</th>\n",
              "      <th>patient</th>\n",
              "      <th>no</th>\n",
              "      <th>she</th>\n",
              "      <th>for</th>\n",
              "      <th>he</th>\n",
              "      <th>were</th>\n",
              "      <th>on</th>\n",
              "      <th>this</th>\n",
              "      <th>at</th>\n",
              "      <th>then</th>\n",
              "      <th>right</th>\n",
              "      <th>or</th>\n",
              "      <th>left</th>\n",
              "      <th>has</th>\n",
              "      <th>as</th>\n",
              "      <th>that</th>\n",
              "      <th>her</th>\n",
              "      <th>history</th>\n",
              "      <th>there</th>\n",
              "      <th>had</th>\n",
              "      <th>his</th>\n",
              "      <th>be</th>\n",
              "      <th>normal</th>\n",
              "      <th>procedure</th>\n",
              "      <th>not</th>\n",
              "      <th>an</th>\n",
              "      <th>placed</th>\n",
              "      <th>1</th>\n",
              "      <th>are</th>\n",
              "      <th>well</th>\n",
              "      <th>i</th>\n",
              "      <th>...</th>\n",
              "      <th>lorcet</th>\n",
              "      <th>atheromatous</th>\n",
              "      <th>wasting</th>\n",
              "      <th>somnolence</th>\n",
              "      <th>tremendous</th>\n",
              "      <th>phisohex</th>\n",
              "      <th>immigrated</th>\n",
              "      <th>itch</th>\n",
              "      <th>anticoagulated</th>\n",
              "      <th>stranded</th>\n",
              "      <th>durable</th>\n",
              "      <th>precarinal</th>\n",
              "      <th>100s</th>\n",
              "      <th>cognition</th>\n",
              "      <th>850</th>\n",
              "      <th>participates</th>\n",
              "      <th>condylectomy</th>\n",
              "      <th>serve</th>\n",
              "      <th>officer</th>\n",
              "      <th>monocular</th>\n",
              "      <th>hematopoietic</th>\n",
              "      <th>talofibular</th>\n",
              "      <th>178</th>\n",
              "      <th>conquer</th>\n",
              "      <th>hydrate</th>\n",
              "      <th>sealing</th>\n",
              "      <th>pericranial</th>\n",
              "      <th>glandular</th>\n",
              "      <th>subconjunctival</th>\n",
              "      <th>rinse</th>\n",
              "      <th>cousin</th>\n",
              "      <th>photodocumentation</th>\n",
              "      <th>emphasized</th>\n",
              "      <th>interaction</th>\n",
              "      <th>stabilizing</th>\n",
              "      <th>radiolucent</th>\n",
              "      <th>l5s</th>\n",
              "      <th>proteins</th>\n",
              "      <th>equivalents</th>\n",
              "      <th>suv</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.009804</td>\n",
              "      <td>0.019608</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.009804</td>\n",
              "      <td>0.009804</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.009804</td>\n",
              "      <td>0.019608</td>\n",
              "      <td>0.009804</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.009804</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.029412</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.039216</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.009804</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.049020</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.009804</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.127208</td>\n",
              "      <td>0.031802</td>\n",
              "      <td>0.056537</td>\n",
              "      <td>0.031802</td>\n",
              "      <td>0.021201</td>\n",
              "      <td>0.014134</td>\n",
              "      <td>0.010601</td>\n",
              "      <td>0.003534</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.003534</td>\n",
              "      <td>0.003534</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.007067</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.007067</td>\n",
              "      <td>0.017668</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.003534</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.007067</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.003534</td>\n",
              "      <td>0.024735</td>\n",
              "      <td>0.010601</td>\n",
              "      <td>0.003534</td>\n",
              "      <td>0.003534</td>\n",
              "      <td>0.003534</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.003534</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.016588</td>\n",
              "      <td>0.035545</td>\n",
              "      <td>0.004739</td>\n",
              "      <td>0.011848</td>\n",
              "      <td>0.014218</td>\n",
              "      <td>0.018957</td>\n",
              "      <td>0.009479</td>\n",
              "      <td>0.009479</td>\n",
              "      <td>0.037915</td>\n",
              "      <td>0.002370</td>\n",
              "      <td>0.014218</td>\n",
              "      <td>0.059242</td>\n",
              "      <td>0.011848</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.002370</td>\n",
              "      <td>0.002370</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.002370</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.004739</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.028436</td>\n",
              "      <td>0.004739</td>\n",
              "      <td>0.011848</td>\n",
              "      <td>0.021327</td>\n",
              "      <td>0.004739</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.009479</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.002370</td>\n",
              "      <td>0.002370</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.011848</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.002370</td>\n",
              "      <td>0.002370</td>\n",
              "      <td>0.004739</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.091575</td>\n",
              "      <td>0.042125</td>\n",
              "      <td>0.038462</td>\n",
              "      <td>0.021978</td>\n",
              "      <td>0.027473</td>\n",
              "      <td>0.012821</td>\n",
              "      <td>0.020147</td>\n",
              "      <td>0.010989</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.020147</td>\n",
              "      <td>0.003663</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.016484</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.010989</td>\n",
              "      <td>0.001832</td>\n",
              "      <td>0.001832</td>\n",
              "      <td>0.009158</td>\n",
              "      <td>0.001832</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.007326</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005495</td>\n",
              "      <td>0.001832</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001832</td>\n",
              "      <td>0.001832</td>\n",
              "      <td>0.020147</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001832</td>\n",
              "      <td>0.010989</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001832</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.027569</td>\n",
              "      <td>0.032581</td>\n",
              "      <td>0.017544</td>\n",
              "      <td>0.050125</td>\n",
              "      <td>0.032581</td>\n",
              "      <td>0.007519</td>\n",
              "      <td>0.027569</td>\n",
              "      <td>0.007519</td>\n",
              "      <td>0.010025</td>\n",
              "      <td>0.005013</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.020050</td>\n",
              "      <td>0.007519</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.002506</td>\n",
              "      <td>0.020050</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.010025</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005013</td>\n",
              "      <td>0.002506</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005013</td>\n",
              "      <td>0.017544</td>\n",
              "      <td>0.007519</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005013</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.015038</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.002506</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005013</td>\n",
              "      <td>0.002506</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.002506</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3995</th>\n",
              "      <td>0.058333</td>\n",
              "      <td>0.033333</td>\n",
              "      <td>0.016667</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.050000</td>\n",
              "      <td>0.025000</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.025000</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.025000</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3996</th>\n",
              "      <td>0.089820</td>\n",
              "      <td>0.041916</td>\n",
              "      <td>0.047904</td>\n",
              "      <td>0.011976</td>\n",
              "      <td>0.041916</td>\n",
              "      <td>0.011976</td>\n",
              "      <td>0.005988</td>\n",
              "      <td>0.023952</td>\n",
              "      <td>0.005988</td>\n",
              "      <td>0.005988</td>\n",
              "      <td>0.023952</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005988</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005988</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005988</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005988</td>\n",
              "      <td>0.017964</td>\n",
              "      <td>0.017964</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005988</td>\n",
              "      <td>0.005988</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3997</th>\n",
              "      <td>0.105063</td>\n",
              "      <td>0.026582</td>\n",
              "      <td>0.054430</td>\n",
              "      <td>0.027848</td>\n",
              "      <td>0.022785</td>\n",
              "      <td>0.027848</td>\n",
              "      <td>0.010127</td>\n",
              "      <td>0.016456</td>\n",
              "      <td>0.001266</td>\n",
              "      <td>0.012658</td>\n",
              "      <td>0.001266</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.007595</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.013924</td>\n",
              "      <td>0.003797</td>\n",
              "      <td>0.002532</td>\n",
              "      <td>0.002532</td>\n",
              "      <td>0.002532</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.002532</td>\n",
              "      <td>0.006329</td>\n",
              "      <td>0.001266</td>\n",
              "      <td>0.001266</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.003797</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.008861</td>\n",
              "      <td>0.001266</td>\n",
              "      <td>0.006329</td>\n",
              "      <td>0.011392</td>\n",
              "      <td>0.001266</td>\n",
              "      <td>0.001266</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001266</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3998</th>\n",
              "      <td>0.027383</td>\n",
              "      <td>0.035497</td>\n",
              "      <td>0.008114</td>\n",
              "      <td>0.016227</td>\n",
              "      <td>0.019270</td>\n",
              "      <td>0.024341</td>\n",
              "      <td>0.013185</td>\n",
              "      <td>0.014199</td>\n",
              "      <td>0.009128</td>\n",
              "      <td>0.017241</td>\n",
              "      <td>0.010142</td>\n",
              "      <td>0.001014</td>\n",
              "      <td>0.015213</td>\n",
              "      <td>0.033469</td>\n",
              "      <td>0.002028</td>\n",
              "      <td>0.004057</td>\n",
              "      <td>0.005071</td>\n",
              "      <td>0.006085</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.014199</td>\n",
              "      <td>0.001014</td>\n",
              "      <td>0.012170</td>\n",
              "      <td>0.002028</td>\n",
              "      <td>0.010142</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.006085</td>\n",
              "      <td>0.001014</td>\n",
              "      <td>0.007099</td>\n",
              "      <td>0.014199</td>\n",
              "      <td>0.001014</td>\n",
              "      <td>0.006085</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.003043</td>\n",
              "      <td>0.004057</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.002028</td>\n",
              "      <td>0.003043</td>\n",
              "      <td>0.001014</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3999</th>\n",
              "      <td>0.129338</td>\n",
              "      <td>0.014196</td>\n",
              "      <td>0.059937</td>\n",
              "      <td>0.026814</td>\n",
              "      <td>0.018927</td>\n",
              "      <td>0.018927</td>\n",
              "      <td>0.009464</td>\n",
              "      <td>0.018927</td>\n",
              "      <td>0.001577</td>\n",
              "      <td>0.004732</td>\n",
              "      <td>0.001577</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.003155</td>\n",
              "      <td>0.001577</td>\n",
              "      <td>0.009464</td>\n",
              "      <td>0.001577</td>\n",
              "      <td>0.003155</td>\n",
              "      <td>0.003155</td>\n",
              "      <td>0.009464</td>\n",
              "      <td>0.001577</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.012618</td>\n",
              "      <td>0.001577</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.003155</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001577</td>\n",
              "      <td>0.003155</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.004732</td>\n",
              "      <td>0.001577</td>\n",
              "      <td>0.003155</td>\n",
              "      <td>0.001577</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.007886</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001577</td>\n",
              "      <td>0.001577</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4000 rows Ã— 10000 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           the       and       was        of  ...  l5s  proteins  equivalents  suv\n",
              "0     0.009804  0.019608  0.000000  0.009804  ...  0.0       0.0          0.0  0.0\n",
              "1     0.127208  0.031802  0.056537  0.031802  ...  0.0       0.0          0.0  0.0\n",
              "2     0.016588  0.035545  0.004739  0.011848  ...  0.0       0.0          0.0  0.0\n",
              "3     0.091575  0.042125  0.038462  0.021978  ...  0.0       0.0          0.0  0.0\n",
              "4     0.027569  0.032581  0.017544  0.050125  ...  0.0       0.0          0.0  0.0\n",
              "...        ...       ...       ...       ...  ...  ...       ...          ...  ...\n",
              "3995  0.058333  0.033333  0.016667  0.008333  ...  0.0       0.0          0.0  0.0\n",
              "3996  0.089820  0.041916  0.047904  0.011976  ...  0.0       0.0          0.0  0.0\n",
              "3997  0.105063  0.026582  0.054430  0.027848  ...  0.0       0.0          0.0  0.0\n",
              "3998  0.027383  0.035497  0.008114  0.016227  ...  0.0       0.0          0.0  0.0\n",
              "3999  0.129338  0.014196  0.059937  0.026814  ...  0.0       0.0          0.0  0.0\n",
              "\n",
              "[4000 rows x 10000 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kfBVkhi5F5IG",
        "outputId": "c91534fc-80bb-48dc-b3c9-f73ca6bf1e33"
      },
      "source": [
        "freq_bgw_valid=freq_bgw(valid_df,list(freq_words_training['Word']))\n",
        "freq_bgw_valid= pd.DataFrame(freq_bgw_valid, columns=freq_words_training['Word'])\n",
        "freq_bgw_valid\n",
        "freq_bgw_valid.to_csv(r'/content/drive/MyDrive/Automn 2021/Machin Learning/Assignment3/freq-bgw_valid.txt', header=True, index=None, sep='\\t')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 499/499 [06:54<00:00,  1.20it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "yYGBaPteGD8x",
        "outputId": "b5c629d7-4647-4bb3-c334-09a785232403"
      },
      "source": [
        "freq_bgw_valid=pd.read_csv(r'/content/drive/MyDrive/Automn 2021/Machin Learning/Assignment3/freq-bgw_valid.txt', sep='\\t')\n",
        "freq_bgw_valid"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>the</th>\n",
              "      <th>and</th>\n",
              "      <th>was</th>\n",
              "      <th>of</th>\n",
              "      <th>to</th>\n",
              "      <th>a</th>\n",
              "      <th>with</th>\n",
              "      <th>in</th>\n",
              "      <th>is</th>\n",
              "      <th>patient</th>\n",
              "      <th>no</th>\n",
              "      <th>she</th>\n",
              "      <th>for</th>\n",
              "      <th>he</th>\n",
              "      <th>were</th>\n",
              "      <th>on</th>\n",
              "      <th>this</th>\n",
              "      <th>at</th>\n",
              "      <th>then</th>\n",
              "      <th>right</th>\n",
              "      <th>or</th>\n",
              "      <th>left</th>\n",
              "      <th>has</th>\n",
              "      <th>as</th>\n",
              "      <th>that</th>\n",
              "      <th>her</th>\n",
              "      <th>history</th>\n",
              "      <th>there</th>\n",
              "      <th>had</th>\n",
              "      <th>his</th>\n",
              "      <th>be</th>\n",
              "      <th>normal</th>\n",
              "      <th>procedure</th>\n",
              "      <th>not</th>\n",
              "      <th>an</th>\n",
              "      <th>placed</th>\n",
              "      <th>1</th>\n",
              "      <th>are</th>\n",
              "      <th>well</th>\n",
              "      <th>i</th>\n",
              "      <th>...</th>\n",
              "      <th>lorcet</th>\n",
              "      <th>atheromatous</th>\n",
              "      <th>wasting</th>\n",
              "      <th>somnolence</th>\n",
              "      <th>tremendous</th>\n",
              "      <th>phisohex</th>\n",
              "      <th>immigrated</th>\n",
              "      <th>itch</th>\n",
              "      <th>anticoagulated</th>\n",
              "      <th>stranded</th>\n",
              "      <th>durable</th>\n",
              "      <th>precarinal</th>\n",
              "      <th>100s</th>\n",
              "      <th>cognition</th>\n",
              "      <th>850</th>\n",
              "      <th>participates</th>\n",
              "      <th>condylectomy</th>\n",
              "      <th>serve</th>\n",
              "      <th>officer</th>\n",
              "      <th>monocular</th>\n",
              "      <th>hematopoietic</th>\n",
              "      <th>talofibular</th>\n",
              "      <th>178</th>\n",
              "      <th>conquer</th>\n",
              "      <th>hydrate</th>\n",
              "      <th>sealing</th>\n",
              "      <th>pericranial</th>\n",
              "      <th>glandular</th>\n",
              "      <th>subconjunctival</th>\n",
              "      <th>rinse</th>\n",
              "      <th>cousin</th>\n",
              "      <th>photodocumentation</th>\n",
              "      <th>emphasized</th>\n",
              "      <th>interaction</th>\n",
              "      <th>stabilizing</th>\n",
              "      <th>radiolucent</th>\n",
              "      <th>l5s</th>\n",
              "      <th>proteins</th>\n",
              "      <th>equivalents</th>\n",
              "      <th>suv</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.082090</td>\n",
              "      <td>0.044776</td>\n",
              "      <td>0.007463</td>\n",
              "      <td>0.022388</td>\n",
              "      <td>0.007463</td>\n",
              "      <td>0.007463</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.007463</td>\n",
              "      <td>0.022388</td>\n",
              "      <td>0.007463</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.007463</td>\n",
              "      <td>0.007463</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.052239</td>\n",
              "      <td>0.014925</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.014925</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.022388</td>\n",
              "      <td>0.007463</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.007463</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.007463</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.007463</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.041667</td>\n",
              "      <td>0.026882</td>\n",
              "      <td>0.016129</td>\n",
              "      <td>0.016129</td>\n",
              "      <td>0.022849</td>\n",
              "      <td>0.016129</td>\n",
              "      <td>0.010753</td>\n",
              "      <td>0.017473</td>\n",
              "      <td>0.013441</td>\n",
              "      <td>0.006720</td>\n",
              "      <td>0.012097</td>\n",
              "      <td>0.016129</td>\n",
              "      <td>0.006720</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.013441</td>\n",
              "      <td>0.005376</td>\n",
              "      <td>0.001344</td>\n",
              "      <td>0.002688</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.006720</td>\n",
              "      <td>0.002688</td>\n",
              "      <td>0.010753</td>\n",
              "      <td>0.010753</td>\n",
              "      <td>0.005376</td>\n",
              "      <td>0.002688</td>\n",
              "      <td>0.008065</td>\n",
              "      <td>0.004032</td>\n",
              "      <td>0.013441</td>\n",
              "      <td>0.009409</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.002688</td>\n",
              "      <td>0.004032</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001344</td>\n",
              "      <td>0.001344</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.002688</td>\n",
              "      <td>0.009409</td>\n",
              "      <td>0.002688</td>\n",
              "      <td>0.001344</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.130726</td>\n",
              "      <td>0.036872</td>\n",
              "      <td>0.043575</td>\n",
              "      <td>0.052514</td>\n",
              "      <td>0.024581</td>\n",
              "      <td>0.020112</td>\n",
              "      <td>0.026816</td>\n",
              "      <td>0.012291</td>\n",
              "      <td>0.003352</td>\n",
              "      <td>0.010056</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001117</td>\n",
              "      <td>0.003352</td>\n",
              "      <td>0.001117</td>\n",
              "      <td>0.003352</td>\n",
              "      <td>0.007821</td>\n",
              "      <td>0.005587</td>\n",
              "      <td>0.011173</td>\n",
              "      <td>0.010056</td>\n",
              "      <td>0.001117</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001117</td>\n",
              "      <td>0.002235</td>\n",
              "      <td>0.001117</td>\n",
              "      <td>0.001117</td>\n",
              "      <td>0.001117</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001117</td>\n",
              "      <td>0.002235</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.012291</td>\n",
              "      <td>0.002235</td>\n",
              "      <td>0.005587</td>\n",
              "      <td>0.005587</td>\n",
              "      <td>0.003352</td>\n",
              "      <td>0.002235</td>\n",
              "      <td>0.001117</td>\n",
              "      <td>0.006704</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.037846</td>\n",
              "      <td>0.032023</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.018923</td>\n",
              "      <td>0.020378</td>\n",
              "      <td>0.018923</td>\n",
              "      <td>0.008734</td>\n",
              "      <td>0.014556</td>\n",
              "      <td>0.014556</td>\n",
              "      <td>0.018923</td>\n",
              "      <td>0.008734</td>\n",
              "      <td>0.023290</td>\n",
              "      <td>0.024745</td>\n",
              "      <td>0.001456</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.002911</td>\n",
              "      <td>0.004367</td>\n",
              "      <td>0.004367</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.004367</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.010189</td>\n",
              "      <td>0.010189</td>\n",
              "      <td>0.002911</td>\n",
              "      <td>0.008734</td>\n",
              "      <td>0.008734</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.002911</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001456</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.002911</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001456</td>\n",
              "      <td>0.001456</td>\n",
              "      <td>0.007278</td>\n",
              "      <td>0.004367</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.103175</td>\n",
              "      <td>0.035714</td>\n",
              "      <td>0.043651</td>\n",
              "      <td>0.023810</td>\n",
              "      <td>0.031746</td>\n",
              "      <td>0.019841</td>\n",
              "      <td>0.039683</td>\n",
              "      <td>0.007937</td>\n",
              "      <td>0.003968</td>\n",
              "      <td>0.015873</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.007937</td>\n",
              "      <td>0.003968</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.003968</td>\n",
              "      <td>0.007937</td>\n",
              "      <td>0.003968</td>\n",
              "      <td>0.007937</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.015873</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.003968</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.003968</td>\n",
              "      <td>0.019841</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.003968</td>\n",
              "      <td>0.011905</td>\n",
              "      <td>0.003968</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.003968</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>494</th>\n",
              "      <td>0.064057</td>\n",
              "      <td>0.035587</td>\n",
              "      <td>0.019573</td>\n",
              "      <td>0.039146</td>\n",
              "      <td>0.003559</td>\n",
              "      <td>0.012456</td>\n",
              "      <td>0.003559</td>\n",
              "      <td>0.012456</td>\n",
              "      <td>0.010676</td>\n",
              "      <td>0.035587</td>\n",
              "      <td>0.028470</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.003559</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.012456</td>\n",
              "      <td>0.001779</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.003559</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.007117</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.008897</td>\n",
              "      <td>0.003559</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.019573</td>\n",
              "      <td>0.012456</td>\n",
              "      <td>0.003559</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.003559</td>\n",
              "      <td>0.014235</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001779</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.010676</td>\n",
              "      <td>0.003559</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005338</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>495</th>\n",
              "      <td>0.053742</td>\n",
              "      <td>0.031322</td>\n",
              "      <td>0.031652</td>\n",
              "      <td>0.021431</td>\n",
              "      <td>0.021101</td>\n",
              "      <td>0.025387</td>\n",
              "      <td>0.006594</td>\n",
              "      <td>0.016156</td>\n",
              "      <td>0.009561</td>\n",
              "      <td>0.001649</td>\n",
              "      <td>0.004616</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.007913</td>\n",
              "      <td>0.047148</td>\n",
              "      <td>0.001978</td>\n",
              "      <td>0.005605</td>\n",
              "      <td>0.002638</td>\n",
              "      <td>0.010551</td>\n",
              "      <td>0.001319</td>\n",
              "      <td>0.000659</td>\n",
              "      <td>0.006264</td>\n",
              "      <td>0.000330</td>\n",
              "      <td>0.003297</td>\n",
              "      <td>0.004616</td>\n",
              "      <td>0.030992</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.003956</td>\n",
              "      <td>0.003627</td>\n",
              "      <td>0.003297</td>\n",
              "      <td>0.008243</td>\n",
              "      <td>0.002967</td>\n",
              "      <td>0.001978</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.007583</td>\n",
              "      <td>0.005275</td>\n",
              "      <td>0.000330</td>\n",
              "      <td>0.000330</td>\n",
              "      <td>0.001319</td>\n",
              "      <td>0.000330</td>\n",
              "      <td>0.000989</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000659</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00033</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00033</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>496</th>\n",
              "      <td>0.092135</td>\n",
              "      <td>0.049438</td>\n",
              "      <td>0.040449</td>\n",
              "      <td>0.026966</td>\n",
              "      <td>0.026966</td>\n",
              "      <td>0.017978</td>\n",
              "      <td>0.011236</td>\n",
              "      <td>0.004494</td>\n",
              "      <td>0.004494</td>\n",
              "      <td>0.015730</td>\n",
              "      <td>0.002247</td>\n",
              "      <td>0.002247</td>\n",
              "      <td>0.013483</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.004494</td>\n",
              "      <td>0.006742</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.008989</td>\n",
              "      <td>0.008989</td>\n",
              "      <td>0.004494</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.004494</td>\n",
              "      <td>0.004494</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.004494</td>\n",
              "      <td>0.006742</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.004494</td>\n",
              "      <td>0.004494</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.004494</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.004494</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.002247</td>\n",
              "      <td>0.004494</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.004494</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>497</th>\n",
              "      <td>0.112418</td>\n",
              "      <td>0.026144</td>\n",
              "      <td>0.058824</td>\n",
              "      <td>0.013072</td>\n",
              "      <td>0.015686</td>\n",
              "      <td>0.006536</td>\n",
              "      <td>0.005229</td>\n",
              "      <td>0.007843</td>\n",
              "      <td>0.001307</td>\n",
              "      <td>0.005229</td>\n",
              "      <td>0.002614</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.006536</td>\n",
              "      <td>0.001307</td>\n",
              "      <td>0.020915</td>\n",
              "      <td>0.005229</td>\n",
              "      <td>0.002614</td>\n",
              "      <td>0.002614</td>\n",
              "      <td>0.018301</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001307</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.002614</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005229</td>\n",
              "      <td>0.001307</td>\n",
              "      <td>0.002614</td>\n",
              "      <td>0.001307</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.013072</td>\n",
              "      <td>0.002614</td>\n",
              "      <td>0.001307</td>\n",
              "      <td>0.001307</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>498</th>\n",
              "      <td>0.014505</td>\n",
              "      <td>0.039249</td>\n",
              "      <td>0.045222</td>\n",
              "      <td>0.019625</td>\n",
              "      <td>0.018771</td>\n",
              "      <td>0.019625</td>\n",
              "      <td>0.012799</td>\n",
              "      <td>0.008532</td>\n",
              "      <td>0.009386</td>\n",
              "      <td>0.000853</td>\n",
              "      <td>0.017918</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.023891</td>\n",
              "      <td>0.029010</td>\n",
              "      <td>0.001706</td>\n",
              "      <td>0.008532</td>\n",
              "      <td>0.010239</td>\n",
              "      <td>0.010239</td>\n",
              "      <td>0.001706</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.013652</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.006826</td>\n",
              "      <td>0.007679</td>\n",
              "      <td>0.007679</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.007679</td>\n",
              "      <td>0.018771</td>\n",
              "      <td>0.005973</td>\n",
              "      <td>0.035836</td>\n",
              "      <td>0.002560</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001706</td>\n",
              "      <td>0.003413</td>\n",
              "      <td>0.002560</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.002560</td>\n",
              "      <td>0.003413</td>\n",
              "      <td>0.000853</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001706</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>499 rows Ã— 10000 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          the       and       was        of  ...  l5s  proteins  equivalents  suv\n",
              "0    0.082090  0.044776  0.007463  0.022388  ...  0.0       0.0          0.0  0.0\n",
              "1    0.041667  0.026882  0.016129  0.016129  ...  0.0       0.0          0.0  0.0\n",
              "2    0.130726  0.036872  0.043575  0.052514  ...  0.0       0.0          0.0  0.0\n",
              "3    0.037846  0.032023  0.000000  0.018923  ...  0.0       0.0          0.0  0.0\n",
              "4    0.103175  0.035714  0.043651  0.023810  ...  0.0       0.0          0.0  0.0\n",
              "..        ...       ...       ...       ...  ...  ...       ...          ...  ...\n",
              "494  0.064057  0.035587  0.019573  0.039146  ...  0.0       0.0          0.0  0.0\n",
              "495  0.053742  0.031322  0.031652  0.021431  ...  0.0       0.0          0.0  0.0\n",
              "496  0.092135  0.049438  0.040449  0.026966  ...  0.0       0.0          0.0  0.0\n",
              "497  0.112418  0.026144  0.058824  0.013072  ...  0.0       0.0          0.0  0.0\n",
              "498  0.014505  0.039249  0.045222  0.019625  ...  0.0       0.0          0.0  0.0\n",
              "\n",
              "[499 rows x 10000 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHWbgItLGL40",
        "outputId": "7ff2ff45-3632-4a51-f4a8-276cb105aeca"
      },
      "source": [
        "freq_bgw_test=freq_bgw(test_df,list(freq_words_training['Word']))\n",
        "freq_bgw_test= pd.DataFrame(freq_bgw_test, columns=freq_words_training['Word'])\n",
        "freq_bgw_test\n",
        "freq_bgw_test.to_csv(r'/content/drive/MyDrive/Automn 2021/Machin Learning/Assignment3/freq-bgw_test.txt', header=True, index=None, sep='\\t')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [06:55<00:00,  1.20it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "9B2BCpoLGSj2",
        "outputId": "1138b620-bf47-49d4-fe34-f210b143e2ea"
      },
      "source": [
        "freq_bgw_test=pd.read_csv(r'/content/drive/MyDrive/Automn 2021/Machin Learning/Assignment3/freq-bgw_test.txt', sep='\\t')\n",
        "freq_bgw_test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>the</th>\n",
              "      <th>and</th>\n",
              "      <th>was</th>\n",
              "      <th>of</th>\n",
              "      <th>to</th>\n",
              "      <th>a</th>\n",
              "      <th>with</th>\n",
              "      <th>in</th>\n",
              "      <th>is</th>\n",
              "      <th>patient</th>\n",
              "      <th>no</th>\n",
              "      <th>she</th>\n",
              "      <th>for</th>\n",
              "      <th>he</th>\n",
              "      <th>were</th>\n",
              "      <th>on</th>\n",
              "      <th>this</th>\n",
              "      <th>at</th>\n",
              "      <th>then</th>\n",
              "      <th>right</th>\n",
              "      <th>or</th>\n",
              "      <th>left</th>\n",
              "      <th>has</th>\n",
              "      <th>as</th>\n",
              "      <th>that</th>\n",
              "      <th>her</th>\n",
              "      <th>history</th>\n",
              "      <th>there</th>\n",
              "      <th>had</th>\n",
              "      <th>his</th>\n",
              "      <th>be</th>\n",
              "      <th>normal</th>\n",
              "      <th>procedure</th>\n",
              "      <th>not</th>\n",
              "      <th>an</th>\n",
              "      <th>placed</th>\n",
              "      <th>1</th>\n",
              "      <th>are</th>\n",
              "      <th>well</th>\n",
              "      <th>i</th>\n",
              "      <th>...</th>\n",
              "      <th>lorcet</th>\n",
              "      <th>atheromatous</th>\n",
              "      <th>wasting</th>\n",
              "      <th>somnolence</th>\n",
              "      <th>tremendous</th>\n",
              "      <th>phisohex</th>\n",
              "      <th>immigrated</th>\n",
              "      <th>itch</th>\n",
              "      <th>anticoagulated</th>\n",
              "      <th>stranded</th>\n",
              "      <th>durable</th>\n",
              "      <th>precarinal</th>\n",
              "      <th>100s</th>\n",
              "      <th>cognition</th>\n",
              "      <th>850</th>\n",
              "      <th>participates</th>\n",
              "      <th>condylectomy</th>\n",
              "      <th>serve</th>\n",
              "      <th>officer</th>\n",
              "      <th>monocular</th>\n",
              "      <th>hematopoietic</th>\n",
              "      <th>talofibular</th>\n",
              "      <th>178</th>\n",
              "      <th>conquer</th>\n",
              "      <th>hydrate</th>\n",
              "      <th>sealing</th>\n",
              "      <th>pericranial</th>\n",
              "      <th>glandular</th>\n",
              "      <th>subconjunctival</th>\n",
              "      <th>rinse</th>\n",
              "      <th>cousin</th>\n",
              "      <th>photodocumentation</th>\n",
              "      <th>emphasized</th>\n",
              "      <th>interaction</th>\n",
              "      <th>stabilizing</th>\n",
              "      <th>radiolucent</th>\n",
              "      <th>l5s</th>\n",
              "      <th>proteins</th>\n",
              "      <th>equivalents</th>\n",
              "      <th>suv</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.048673</td>\n",
              "      <td>0.039823</td>\n",
              "      <td>0.022124</td>\n",
              "      <td>0.039823</td>\n",
              "      <td>0.026549</td>\n",
              "      <td>0.004425</td>\n",
              "      <td>0.008850</td>\n",
              "      <td>0.004425</td>\n",
              "      <td>0.004425</td>\n",
              "      <td>0.022124</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.013274</td>\n",
              "      <td>0.004425</td>\n",
              "      <td>0.004425</td>\n",
              "      <td>0.013274</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.022124</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.004425</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.008850</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.008850</td>\n",
              "      <td>0.00885</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.004425</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.004425</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.004425</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.090909</td>\n",
              "      <td>0.030948</td>\n",
              "      <td>0.040619</td>\n",
              "      <td>0.034816</td>\n",
              "      <td>0.021277</td>\n",
              "      <td>0.009671</td>\n",
              "      <td>0.021277</td>\n",
              "      <td>0.017408</td>\n",
              "      <td>0.007737</td>\n",
              "      <td>0.015474</td>\n",
              "      <td>0.001934</td>\n",
              "      <td>0.001934</td>\n",
              "      <td>0.007737</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.003868</td>\n",
              "      <td>0.009671</td>\n",
              "      <td>0.001934</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.015474</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.017408</td>\n",
              "      <td>0.007737</td>\n",
              "      <td>0.007737</td>\n",
              "      <td>0.001934</td>\n",
              "      <td>0.001934</td>\n",
              "      <td>0.001934</td>\n",
              "      <td>0.001934</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.003868</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005803</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.007737</td>\n",
              "      <td>0.003868</td>\n",
              "      <td>0.003868</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.003868</td>\n",
              "      <td>0.001934</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.039568</td>\n",
              "      <td>0.032374</td>\n",
              "      <td>0.025180</td>\n",
              "      <td>0.035971</td>\n",
              "      <td>0.007194</td>\n",
              "      <td>0.035971</td>\n",
              "      <td>0.014388</td>\n",
              "      <td>0.010791</td>\n",
              "      <td>0.017986</td>\n",
              "      <td>0.003597</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.025180</td>\n",
              "      <td>0.010791</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.010791</td>\n",
              "      <td>0.007194</td>\n",
              "      <td>0.010791</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.003597</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.003597</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.014388</td>\n",
              "      <td>0.003597</td>\n",
              "      <td>0.003597</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.003597</td>\n",
              "      <td>0.007194</td>\n",
              "      <td>0.003597</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.007194</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.007194</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.007194</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.081290</td>\n",
              "      <td>0.042581</td>\n",
              "      <td>0.064516</td>\n",
              "      <td>0.014194</td>\n",
              "      <td>0.012903</td>\n",
              "      <td>0.014194</td>\n",
              "      <td>0.014194</td>\n",
              "      <td>0.023226</td>\n",
              "      <td>0.002581</td>\n",
              "      <td>0.015484</td>\n",
              "      <td>0.002581</td>\n",
              "      <td>0.003871</td>\n",
              "      <td>0.006452</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.010323</td>\n",
              "      <td>0.010323</td>\n",
              "      <td>0.009032</td>\n",
              "      <td>0.001290</td>\n",
              "      <td>0.002581</td>\n",
              "      <td>0.011613</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001290</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.016774</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.003871</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.003871</td>\n",
              "      <td>0.002581</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005161</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001290</td>\n",
              "      <td>0.015484</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.014194</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.033088</td>\n",
              "      <td>0.029412</td>\n",
              "      <td>0.003676</td>\n",
              "      <td>0.025735</td>\n",
              "      <td>0.029412</td>\n",
              "      <td>0.007353</td>\n",
              "      <td>0.003676</td>\n",
              "      <td>0.018382</td>\n",
              "      <td>0.011029</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.014706</td>\n",
              "      <td>0.025735</td>\n",
              "      <td>0.003676</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.007353</td>\n",
              "      <td>0.003676</td>\n",
              "      <td>0.003676</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.003676</td>\n",
              "      <td>0.007353</td>\n",
              "      <td>0.018382</td>\n",
              "      <td>0.014706</td>\n",
              "      <td>0.011029</td>\n",
              "      <td>0.011029</td>\n",
              "      <td>0.025735</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.003676</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.003676</td>\n",
              "      <td>0.007353</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>495</th>\n",
              "      <td>0.037791</td>\n",
              "      <td>0.037791</td>\n",
              "      <td>0.042151</td>\n",
              "      <td>0.021802</td>\n",
              "      <td>0.017442</td>\n",
              "      <td>0.029070</td>\n",
              "      <td>0.018895</td>\n",
              "      <td>0.007267</td>\n",
              "      <td>0.010174</td>\n",
              "      <td>0.004360</td>\n",
              "      <td>0.010174</td>\n",
              "      <td>0.023256</td>\n",
              "      <td>0.008721</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.010174</td>\n",
              "      <td>0.017442</td>\n",
              "      <td>0.014535</td>\n",
              "      <td>0.015988</td>\n",
              "      <td>0.001453</td>\n",
              "      <td>0.005814</td>\n",
              "      <td>0.001453</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001453</td>\n",
              "      <td>0.007267</td>\n",
              "      <td>0.005814</td>\n",
              "      <td>0.002907</td>\n",
              "      <td>0.001453</td>\n",
              "      <td>0.005814</td>\n",
              "      <td>0.002907</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.005814</td>\n",
              "      <td>0.004360</td>\n",
              "      <td>0.001453</td>\n",
              "      <td>0.001453</td>\n",
              "      <td>0.004360</td>\n",
              "      <td>0.002907</td>\n",
              "      <td>0.001453</td>\n",
              "      <td>0.004360</td>\n",
              "      <td>0.004360</td>\n",
              "      <td>0.004360</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>496</th>\n",
              "      <td>0.033613</td>\n",
              "      <td>0.042017</td>\n",
              "      <td>0.008403</td>\n",
              "      <td>0.033613</td>\n",
              "      <td>0.025210</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.008403</td>\n",
              "      <td>0.008403</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.008403</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.008403</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.033613</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.008403</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.008403</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.008403</td>\n",
              "      <td>0.008403</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.008403</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>497</th>\n",
              "      <td>0.123418</td>\n",
              "      <td>0.053797</td>\n",
              "      <td>0.034810</td>\n",
              "      <td>0.031646</td>\n",
              "      <td>0.018987</td>\n",
              "      <td>0.018987</td>\n",
              "      <td>0.006329</td>\n",
              "      <td>0.022152</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.006329</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.009494</td>\n",
              "      <td>0.006329</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.015823</td>\n",
              "      <td>0.006329</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.006329</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.003165</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.012658</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.003165</td>\n",
              "      <td>0.015823</td>\n",
              "      <td>0.003165</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.003165</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>498</th>\n",
              "      <td>0.050233</td>\n",
              "      <td>0.031628</td>\n",
              "      <td>0.001860</td>\n",
              "      <td>0.028837</td>\n",
              "      <td>0.016744</td>\n",
              "      <td>0.017674</td>\n",
              "      <td>0.004651</td>\n",
              "      <td>0.005581</td>\n",
              "      <td>0.038140</td>\n",
              "      <td>0.018605</td>\n",
              "      <td>0.014884</td>\n",
              "      <td>0.041860</td>\n",
              "      <td>0.008372</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000930</td>\n",
              "      <td>0.005581</td>\n",
              "      <td>0.006512</td>\n",
              "      <td>0.002791</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000930</td>\n",
              "      <td>0.016744</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.020465</td>\n",
              "      <td>0.005581</td>\n",
              "      <td>0.003721</td>\n",
              "      <td>0.014884</td>\n",
              "      <td>0.003721</td>\n",
              "      <td>0.007442</td>\n",
              "      <td>0.005581</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.009302</td>\n",
              "      <td>0.001860</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.003721</td>\n",
              "      <td>0.001860</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.002791</td>\n",
              "      <td>0.006512</td>\n",
              "      <td>0.001860</td>\n",
              "      <td>0.003721</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>499</th>\n",
              "      <td>0.128272</td>\n",
              "      <td>0.039267</td>\n",
              "      <td>0.062827</td>\n",
              "      <td>0.023560</td>\n",
              "      <td>0.026178</td>\n",
              "      <td>0.015707</td>\n",
              "      <td>0.015707</td>\n",
              "      <td>0.013089</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005236</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005236</td>\n",
              "      <td>0.002618</td>\n",
              "      <td>0.005236</td>\n",
              "      <td>0.002618</td>\n",
              "      <td>0.026178</td>\n",
              "      <td>0.005236</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005236</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005236</td>\n",
              "      <td>0.007853</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.002618</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>500 rows Ã— 10000 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          the       and       was        of  ...  l5s  proteins  equivalents  suv\n",
              "0    0.048673  0.039823  0.022124  0.039823  ...  0.0       0.0          0.0  0.0\n",
              "1    0.090909  0.030948  0.040619  0.034816  ...  0.0       0.0          0.0  0.0\n",
              "2    0.039568  0.032374  0.025180  0.035971  ...  0.0       0.0          0.0  0.0\n",
              "3    0.081290  0.042581  0.064516  0.014194  ...  0.0       0.0          0.0  0.0\n",
              "4    0.033088  0.029412  0.003676  0.025735  ...  0.0       0.0          0.0  0.0\n",
              "..        ...       ...       ...       ...  ...  ...       ...          ...  ...\n",
              "495  0.037791  0.037791  0.042151  0.021802  ...  0.0       0.0          0.0  0.0\n",
              "496  0.033613  0.042017  0.008403  0.033613  ...  0.0       0.0          0.0  0.0\n",
              "497  0.123418  0.053797  0.034810  0.031646  ...  0.0       0.0          0.0  0.0\n",
              "498  0.050233  0.031628  0.001860  0.028837  ...  0.0       0.0          0.0  0.0\n",
              "499  0.128272  0.039267  0.062827  0.023560  ...  0.0       0.0          0.0  0.0\n",
              "\n",
              "[500 rows x 10000 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-FjKWMS5rqZ",
        "outputId": "4323fe0a-fe3f-4655-a11c-a800b14219f8"
      },
      "source": [
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "freq_bgw_scrMatrix=csr_matrix(freq_bgw.values)\n",
        "freq_bgw_scrMatrix"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<4000x10000 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 3511981 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {},
          "execution_count": 154
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4Qoguu7bmLZ"
      },
      "source": [
        "import tqdm\n",
        "\n",
        "def bgw_transform(vocabulary,df):\n",
        "  bgw=[]\n",
        "  freq_word=list(vocabulary['Word'])\n",
        "  nbr_word=vocabulary['Word'].shape[0]\n",
        "  for row in tqdm.tqdm(df[\"words\"]):\n",
        "    bg=[]\n",
        "    for word in row:\n",
        "      if word in freq_word:\n",
        "        bg.append(int(vocabulary[vocabulary['Word']==word]['id']))\n",
        "    bg1=' '.join(map(str, bg))\n",
        "    bgw.append(bg1)\n",
        "  return(bgw)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YxpS-O2ucYPg",
        "outputId": "4c14d385-b000-4d1e-8520-421c4a14f9d5"
      },
      "source": [
        "medical_text_train=bgw_transform(freq_words_training, train_df)\n",
        "medical_text_train= pd.DataFrame(medical_text_train)\n",
        "medical_text_train[1]=train_df['label']\n",
        "medical_text_train.to_csv(r'/content/drive/MyDrive/Automn 2021/Machin Learning/Assignment3/medical_text-train.txt', header=False, index=None, sep='\\t')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4000/4000 [46:18<00:00,  1.44it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sPofAGWcpsQn",
        "outputId": "828237ef-4e65-459e-c822-b2cc3071ec44"
      },
      "source": [
        "medical_text_valid=bgw_transform(freq_words_training, valid_df)\n",
        "medical_text_valid= pd.DataFrame(medical_text_valid)\n",
        "medical_text_valid[1]=valid_df['label']\n",
        "medical_text_valid.to_csv(r'/content/drive/MyDrive/Automn 2021/Machin Learning/Assignment3/medical_text-valid.txt', header=False, index=None, sep='\\t')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 499/499 [05:51<00:00,  1.42it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVLbqS4ErR6P",
        "outputId": "da70f36e-4347-4ae5-f86d-945a7fc738c4"
      },
      "source": [
        "medical_text_test=bgw_transform(freq_words_training, test_df)\n",
        "medical_text_test= pd.DataFrame(medical_text_test)\n",
        "medical_text_test[1]=test_df['label']\n",
        "medical_text_test.to_csv(r'/content/drive/MyDrive/Automn 2021/Machin Learning/Assignment3/medical_text-test.txt', header=False, index=None, sep='\\t')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [05:57<00:00,  1.40it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBhAI67DhrGH"
      },
      "source": [
        "import sklearn\n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.naive_bayes import  BernoulliNB\n",
        "from sklearn.naive_bayes import  GaussianNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import PredefinedSplit"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0PtLDovXIdc"
      },
      "source": [
        "#merge train and validation dataset \n",
        "train_df_total_label=list(pd.concat([train_df['label'],valid_df['label']], ignore_index=True))\n",
        "binary_bgw_total=pd.concat([binary_bgw_train,binary_bgw_valid], ignore_index=True)\n",
        "binary_bgw_total['label']=train_df_total_label\n",
        "# train_df_total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Zr-BU3MKV8H"
      },
      "source": [
        "#merge train and validation dataset \n",
        "train_df_total_label=list(pd.concat([train_df['label'],valid_df['label']], ignore_index=True))\n",
        "freq_bgw_total=pd.concat([freq_bgw_train,freq_bgw_valid], ignore_index=True)\n",
        "freq_bgw_total['label']=train_df_total_label\n",
        "# train_df_total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxCSroV2xnmB"
      },
      "source": [
        "# 2.a"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JpWXY5VxKIA"
      },
      "source": [
        "#labeling based on uniformly random class\n",
        "import random\n",
        "def random_labeling(N):\n",
        "  label_pred=[]\n",
        "  for i in range (N):\n",
        "    label=0\n",
        "    x=random.uniform(0, 1)\n",
        "    if 0<=x<=0.25:\n",
        "      label=1\n",
        "    elif 0.25<x<=0.5:\n",
        "      label=2\n",
        "    elif 0.5<x<=0.75:\n",
        "      label=3\n",
        "    elif 0.75<x<=1:\n",
        "      label=4\n",
        "    label_pred.append(label)\n",
        "  return label_pred\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDAOklU2zJzg"
      },
      "source": [
        "label_random_train=random_labeling(train_df['label'].shape[0])\n",
        "label_random_valid=random_labeling(valid_df['label'].shape[0])\n",
        "label_random_test=random_labeling(test_df['label'].shape[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UG0Yuh0uVzWf"
      },
      "source": [
        "def calc_results(pred_class, true_class):\n",
        "  rnd_result={}\n",
        "  for i in set(true_class):\n",
        "    metric={'TP':0,'TN':0,'FP':0,'FN':0}\n",
        "    for j,label in enumerate(pred_class):\n",
        "      if label==true_class[j]==i:\n",
        "        metric['TP']+=1\n",
        "      elif label!=true_class[j]==i:\n",
        "        metric['FN']+=1\n",
        "      elif true_class[j]!=label==i:\n",
        "        metric['FP']+=1\n",
        "      else:\n",
        "        metric['TN']+=1\n",
        "    rnd_result[i]=metric\n",
        "  return rnd_result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dRX1f8neWb2M",
        "outputId": "330162e8-0a81-4409-fb81-3666c6128bc5"
      },
      "source": [
        "calc_results(label_random_train, list(train_df['label']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{1: {'FN': 955, 'FP': 640, 'TN': 2083, 'TP': 322},\n",
              " 2: {'FN': 736, 'FP': 744, 'TN': 2248, 'TP': 272},\n",
              " 3: {'FN': 719, 'FP': 778, 'TN': 2288, 'TP': 215},\n",
              " 4: {'FN': 575, 'FP': 823, 'TN': 2396, 'TP': 206}}"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgod0heKOvgp",
        "outputId": "a031e2bb-e150-4955-8fcf-3b5334344ec4"
      },
      "source": [
        "f1_train = f1_score( list(train_df['label']),label_random_train, average='macro')\n",
        "f1_valid = f1_score( list(valid_df['label']),label_random_valid, average='macro')\n",
        "f1_test = f1_score( list(test_df['label']),label_random_test, average='macro')\n",
        "print(f'F1-score of random labeling\\n Training: {f1_train}\\n Vlidation: {f1_valid} \\n Test:{f1_test}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1-score of random labeling\n",
            " Training: 0.25179305078177494\n",
            " Vlidation: 0.2643624871062423 \n",
            " Test:0.24236153846045608\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Ij_UhTAxrne",
        "outputId": "e403d3d5-7330-45ec-8e8a-f719f29a0265"
      },
      "source": [
        "#labeling based on uniformly majority class\n",
        "label_count_train=train_df.groupby(train_df['label']).agg({'words':['count'] })\n",
        "label_count_train.columns=['label_count']\n",
        "label_count_train=label_count_train.reset_index()\n",
        "majority_class=int(label_count_train.loc[label_count_train['label_count']==label_count_train['label_count'].max()]['label'])\n",
        "print(label_count_train)\n",
        "print(f'The majority class for Medical dataset is: {majority_class}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   label  label_count\n",
            "0      1         1277\n",
            "1      2         1008\n",
            "2      3          934\n",
            "3      4          781\n",
            "The majority class for Medical dataset is: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "643tZ0ERNKgU"
      },
      "source": [
        "label_maj_train=[majority_class]*train_df['label'].shape[0]\n",
        "label_maj_valid=[majority_class]*valid_df['label'].shape[0]\n",
        "label_maj_test=[majority_class]*test_df['label'].shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZMqvXETX35_",
        "outputId": "1b1f7c8b-0518-4cec-9a94-e89bea66dc4c"
      },
      "source": [
        "calc_results(label_maj_train, list(train_df['label']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{1: {'FN': 0, 'FP': 2723, 'TN': 0, 'TP': 1277},\n",
              " 2: {'FN': 1008, 'FP': 0, 'TN': 2992, 'TP': 0},\n",
              " 3: {'FN': 934, 'FP': 0, 'TN': 3066, 'TP': 0},\n",
              " 4: {'FN': 781, 'FP': 0, 'TN': 3219, 'TP': 0}}"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lb8hch8AT5hd",
        "outputId": "9a02636d-d096-4a15-92c3-62c3e812b4b1"
      },
      "source": [
        "f1_train = f1_score( list(train_df['label']),label_maj_train, average='macro')\n",
        "f1_valid = f1_score( list(valid_df['label']),label_maj_valid, average='macro')\n",
        "f1_test = f1_score( list(test_df['label']),label_maj_test, average='macro')\n",
        "print(f'F1-score of majority class labeling\\n Training: {f1_train}\\n Vlidation: {f1_valid} \\n Test:{f1_test}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1-score of majority class labeling\n",
            " Training: 0.120996778472617\n",
            " Vlidation: 0.12424698795180723 \n",
            " Test:0.14183381088825217\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjggcDgTD7Yw"
      },
      "source": [
        "# 2.b-Bernouli Naive Bayes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoCnnMhXpI_u"
      },
      "source": [
        "In the case of text classification, word occurrence vectors (rather than word count vectors) may be used to train and use this classifier. BernoulliNB might perform better on some datasets, especially those with shorter documents. It is advisable to evaluate both models, if time permits."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sv5yX4IS-sWw"
      },
      "source": [
        "# import numpy as np\n",
        "# train_total = train_df_total.to_numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-nXPJKBTcxm",
        "outputId": "719fb02a-4e2b-4a01-d664-f71d86cd8d04"
      },
      "source": [
        "clf = BernoulliNB()\n",
        "clf.fit(binary_bgw_train, train_df['label'])\n",
        "f1_train = f1_score( list(train_df['label']),clf.predict(binary_bgw_train), average='macro')\n",
        "f1_valid = f1_score( list(valid_df['label']),clf.predict(binary_bgw_valid), average='macro')\n",
        "f1_test = f1_score( list(test_df['label']),clf.predict(binary_bgw_test), average='macro')\n",
        "print(f'F1-score of  Bernouli Naive Bayes without hyperparameter tunning\\n Training: {f1_train}\\n Vlidation: {f1_valid} \\n Test:{f1_test}')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1-score of  Bernouli Naive Bayes without hyperparameter tunning\n",
            " Training: 0.5335748842966954\n",
            " Vlidation: 0.4586511509873923 \n",
            " Test:0.46750737765273176\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFo3mhHR-SZa",
        "outputId": "bbd8fea4-425d-41d6-c4e8-53674ca68b3d"
      },
      "source": [
        "# clf = BernoulliNB()\n",
        "# clf.fit(train_df_total.iloc[:,:10000], train_df_total['label'])\n",
        "params = {'alpha': np.arange(0, 1.005, 0.005)}\n",
        "f = [-1 for i in range(train_df.shape[0])] + [1 for i in range(valid_df.shape[0])]\n",
        "p= PredefinedSplit(test_fold = f)\n",
        "clf_bnb = GridSearchCV(BernoulliNB(), param_grid=params, cv=p, scoring='f1_macro')\n",
        "clf_bnb.fit(binary_bgw_total.iloc[:,:10000], binary_bgw_total['label'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/naive_bayes.py:507: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
            "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=PredefinedSplit(test_fold=array([-1, -1, ...,  1,  1])),\n",
              "             error_score=nan,\n",
              "             estimator=BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None,\n",
              "                                   fit_prior=True),\n",
              "             iid='deprecated', n_jobs=None,\n",
              "             param_grid={'alpha': array([0.   , 0.005, 0.01 , 0.015, 0.02 , 0.025, 0.03 , 0.035, 0.04 ,\n",
              "       0.045, 0.05 , 0.055, 0.06 , 0.065, 0.07 , 0.075, 0.08 , 0.085,\n",
              "       0.09 , 0.095, 0.1  , 0.105, 0.11 , 0.115, 0.12 ,...\n",
              "       0.765, 0.77 , 0.775, 0.78 , 0.785, 0.79 , 0.795, 0.8  , 0.805,\n",
              "       0.81 , 0.815, 0.82 , 0.825, 0.83 , 0.835, 0.84 , 0.845, 0.85 ,\n",
              "       0.855, 0.86 , 0.865, 0.87 , 0.875, 0.88 , 0.885, 0.89 , 0.895,\n",
              "       0.9  , 0.905, 0.91 , 0.915, 0.92 , 0.925, 0.93 , 0.935, 0.94 ,\n",
              "       0.945, 0.95 , 0.955, 0.96 , 0.965, 0.97 , 0.975, 0.98 , 0.985,\n",
              "       0.99 , 0.995, 1.   ])},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring='f1_macro', verbose=0)"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mi8FOQl00tyB",
        "outputId": "68f994de-7adf-479a-949e-4b22223890bd"
      },
      "source": [
        "clf_bnb.best_params_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'alpha': 0.8150000000000001}"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UMNH0aD4v1G",
        "outputId": "f3ff4975-bd40-42b1-dd6c-1457d4f3b551"
      },
      "source": [
        "f1_train = f1_score( list(train_df['label']),clf_bnb.predict(binary_bgw_train), average='macro')\n",
        "f1_valid = f1_score( list(valid_df['label']),clf_bnb.predict(binary_bgw_valid), average='macro')\n",
        "f1_test = f1_score( list(test_df['label']),clf_bnb.predict(binary_bgw_test), average='macro')\n",
        "print(f'F1-score of  Bernouli Naive Bayes with alpha={clf_bnb.best_params_}\\n Training: {f1_train}\\n Vlidation: {f1_valid} \\n Test:{f1_test}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1-score of  Bernouli Naive Bayes with alpha={'alpha': 0.8150000000000001}\n",
            " Training: 0.5318753352433206\n",
            " Vlidation: 0.49798012032813965 \n",
            " Test:0.47337876955592384\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgKq5L2akSVP"
      },
      "source": [
        "# 2.b Decision Trees"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROycfQ0gw9c0",
        "outputId": "110f6b83-0759-40d0-89e3-e970c97f141e"
      },
      "source": [
        "clf_dec=DecisionTreeClassifier()\n",
        "clf_dec.fit(binary_bgw_train, train_df['label'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
              "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
              "                       random_state=None, splitter='best')"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N3cn1QKXkR_L",
        "outputId": "a869617e-6fee-4bb4-adcd-4bb8a472c4bf"
      },
      "source": [
        "f1_train = f1_score( list(train_df['label']),clf_dec.predict(binary_bgw_train), average='macro')\n",
        "f1_valid = f1_score( list(valid_df['label']),clf_dec.predict(binary_bgw_valid), average='macro')\n",
        "f1_test = f1_score( list(test_df['label']),clf_dec.predict(binary_bgw_test), average='macro')\n",
        "print(f'F1-score of Decision Tree without hyperparameter tunning\\n Training: {f1_train}\\n Validation: {f1_valid} \\n Test:{f1_test}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1-score of Decision Tree without hyperparameter tunning\n",
            " Training: 0.907376931421898\n",
            " Validation: 0.7096489149799332 \n",
            " Test:0.740212028565679\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ecc1JdAqkbYv",
        "outputId": "49f80c40-f26e-4c6f-e6f7-63a39fb386d9"
      },
      "source": [
        "# params = {'max_depth': np.arange(17, 20),'ccp_alpha': np.arange(0, .005, .001), 'max_features': np.arange(6000, 10001, 1000), 'max_leaf_nodes': np.arange(1500, 3000, 250), 'min_samples_leaf': np.arange(3, 7)}\n",
        "params = {'ccp_alpha': np.arange(0, .005, .001), 'min_samples_leaf': np.arange(3, 10)}\n",
        "f = [-1 for i in range(train_df.shape[0])] + [1 for i in range(valid_df.shape[0])]\n",
        "p= PredefinedSplit(test_fold = f)\n",
        "clf_dec_h = GridSearchCV(DecisionTreeClassifier(), param_grid=params, cv=p, scoring='f1_macro')\n",
        "clf_dec_h.fit(binary_bgw_total.iloc[:,:10000], binary_bgw_total['label'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=PredefinedSplit(test_fold=array([-1, -1, ...,  1,  1])),\n",
              "             error_score=nan,\n",
              "             estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,\n",
              "                                              criterion='gini', max_depth=None,\n",
              "                                              max_features=None,\n",
              "                                              max_leaf_nodes=None,\n",
              "                                              min_impurity_decrease=0.0,\n",
              "                                              min_impurity_split=None,\n",
              "                                              min_samples_leaf=1,\n",
              "                                              min_samples_split=2,\n",
              "                                              min_weight_fraction_leaf=0.0,\n",
              "                                              presort='deprecated',\n",
              "                                              random_state=None,\n",
              "                                              splitter='best'),\n",
              "             iid='deprecated', n_jobs=None,\n",
              "             param_grid={'ccp_alpha': array([0.   , 0.001, 0.002, 0.003, 0.004]),\n",
              "                         'min_samples_leaf': array([3, 4, 5, 6, 7, 8, 9])},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring='f1_macro', verbose=0)"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7uvylvIq68Mp",
        "outputId": "a4e3420a-bf71-403d-b7e9-078b6eeb3540"
      },
      "source": [
        "clf_dec_h.best_params_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'ccp_alpha': 0.001, 'min_samples_leaf': 6}"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dnmMcYM21qjE",
        "outputId": "7bd7a1e7-7495-4408-8865-38cd3f6ba30c"
      },
      "source": [
        "f1_train = f1_score( list(train_df['label']),clf_dec_h.predict(binary_bgw_train), average='macro')\n",
        "f1_valid = f1_score( list(valid_df['label']),clf_dec_h.predict(binary_bgw_valid), average='macro')\n",
        "f1_test = f1_score( list(test_df['label']),clf_dec_h.predict(binary_bgw_test), average='macro')\n",
        "print(f'F1-score of Decision Tree with hyperparameter tunning\\n Training: {f1_train}\\n Validation: {f1_valid} \\n Test:{f1_test}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1-score of Decision Tree with hyperparameter tunning\n",
            " Training: 0.8089879709867438\n",
            " Validation: 0.8010233707952481 \n",
            " Test:0.798325636663274\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoI6FR7she_N"
      },
      "source": [
        "# 2.b Logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nO0dOIyNhfpP",
        "outputId": "0da0a530-eae2-49fb-a6d8-1034bf1bfd57"
      },
      "source": [
        "clf_lr = LogisticRegression().fit(binary_bgw_train, train_df['label'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mijikf_7iDfK",
        "outputId": "08420d85-a23a-4403-afef-68bb468dfb26"
      },
      "source": [
        "f1_train = f1_score( list(train_df['label']),clf_lr.predict(binary_bgw_train), average='macro')\n",
        "f1_valid = f1_score( list(valid_df['label']),clf_lr.predict(binary_bgw_valid), average='macro')\n",
        "f1_test = f1_score( list(test_df['label']),clf_lr.predict(binary_bgw_test), average='macro')\n",
        "print(f'F1-score of Logistic Regression without hyperparameter tunning\\n Training: {f1_train}\\n Validation: {f1_valid} \\n Test:{f1_test}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1-score of Logistic Regression without hyperparameter tunning\n",
            " Training: 0.9083094313212405\n",
            " Validation: 0.6851153427007235 \n",
            " Test:0.722191909688582\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jw_wcmB2kyoP",
        "outputId": "e374d294-2189-474e-fa06-12e34459bac8"
      },
      "source": [
        "params = {'penalty':['l1','l2'], 'C':np.arange(0.2, 0.6, .2), 'solver':['liblinear', 'saga']}\n",
        "# params = {'penalty':['l1'], 'solver':['liblinear'], 'C':np.arange(.2, .4, .1)}\n",
        "f = [-1 for i in range(train_df.shape[0])] + [1 for i in range(valid_df.shape[0])]\n",
        "p= PredefinedSplit(test_fold = f)\n",
        "clf_rl_h = GridSearchCV(LogisticRegression(), param_grid=params, cv=p, scoring='f1_macro')\n",
        "clf_rl_h.fit(binary_bgw_total.iloc[:,:10000], binary_bgw_total['label'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=PredefinedSplit(test_fold=array([-1, -1, ...,  1,  1])),\n",
              "             error_score=nan,\n",
              "             estimator=LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
              "                                          fit_intercept=True,\n",
              "                                          intercept_scaling=1, l1_ratio=None,\n",
              "                                          max_iter=100, multi_class='auto',\n",
              "                                          n_jobs=None, penalty='l2',\n",
              "                                          random_state=None, solver='lbfgs',\n",
              "                                          tol=0.0001, verbose=0,\n",
              "                                          warm_start=False),\n",
              "             iid='deprecated', n_jobs=None,\n",
              "             param_grid={'C': array([0.2, 0.4]), 'penalty': ['l1', 'l2'],\n",
              "                         'solver': ['liblinear', 'saga']},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring='f1_macro', verbose=0)"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acFmrXYPk9si",
        "outputId": "3fc1886a-1c80-424f-f5d8-80faf9e66ffa"
      },
      "source": [
        "clf_rl_h.best_params_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'C': 0.2, 'penalty': 'l1', 'solver': 'liblinear'}"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njEIWvcwlBWb",
        "outputId": "3094bdd8-3956-42df-e624-55b1bee2d9bf"
      },
      "source": [
        "f1_train = f1_score( list(train_df['label']),clf_rl_h.predict(binary_bgw_train), average='macro')\n",
        "f1_valid = f1_score( list(valid_df['label']),clf_rl_h.predict(binary_bgw_valid), average='macro')\n",
        "f1_test = f1_score( list(test_df['label']),clf_rl_h.predict(binary_bgw_test), average='macro')\n",
        "print(f'F1-score of Logistic Regression with hyperparameter tunning\\n Training: {f1_train}\\n Validation: {f1_valid} \\n Test:{f1_test}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1-score of Logistic Regression with hyperparameter tunning\n",
            " Training: 0.8618431127019649\n",
            " Validation: 0.8572274533436299 \n",
            " Test:0.7940420793255759\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5NSMoAiv4xl"
      },
      "source": [
        "# 2.b Linear SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tT1L7Vosv9cQ",
        "outputId": "ba1d76c3-391a-4709-92df-e91cb25830d1"
      },
      "source": [
        "clf_svm = LinearSVC().fit(binary_bgw_train, train_df['label'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2MjOWbxfwCAE",
        "outputId": "e1e56f64-0f08-4f48-abeb-a4d2e83c193b"
      },
      "source": [
        "f1_train = f1_score( list(train_df['label']),clf_svm.predict(binary_bgw_train), average='macro')\n",
        "f1_valid = f1_score( list(valid_df['label']),clf_svm.predict(binary_bgw_valid), average='macro')\n",
        "f1_test = f1_score( list(test_df['label']),clf_svm.predict(binary_bgw_test), average='macro')\n",
        "print(f'F1-score of Linear SVM without hyperparameter tunning\\n Training: {f1_train}\\n Validation: {f1_valid} \\n Test:{f1_test}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1-score of Linear SVM without hyperparameter tunning\n",
            " Training: 0.9076197296518124\n",
            " Validation: 0.7415093629873268 \n",
            " Test:0.7776470979223433\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJbcObvFwEgM",
        "outputId": "ea7b573d-2223-45a7-d976-edba529f41df"
      },
      "source": [
        "params = {'penalty':['l1','l2'], 'C':np.arange(0.1, 0.6, 0.1), 'loss':['hinge', 'squared_hinge'], 'dual':[True,False]}\n",
        "# params = {'penalty':['l1', 'l2'], 'loss':['hinge', 'squared_hinge']}\n",
        "f = [-1 for i in range(train_df.shape[0])] + [1 for i in range(valid_df.shape[0])]\n",
        "p= PredefinedSplit(test_fold = f)\n",
        "clf_svm_h = GridSearchCV(LinearSVC(), param_grid=params, cv=p)\n",
        "clf_svm_h.fit(binary_bgw_total.iloc[:,:10000], binary_bgw_total['label'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=PredefinedSplit(test_fold=array([-1, -1, ...,  1,  1])),\n",
              "             error_score=nan,\n",
              "             estimator=LinearSVC(C=1.0, class_weight=None, dual=True,\n",
              "                                 fit_intercept=True, intercept_scaling=1,\n",
              "                                 loss='squared_hinge', max_iter=1000,\n",
              "                                 multi_class='ovr', penalty='l2',\n",
              "                                 random_state=None, tol=0.0001, verbose=0),\n",
              "             iid='deprecated', n_jobs=None,\n",
              "             param_grid={'C': array([0.1, 0.2, 0.3, 0.4, 0.5]), 'dual': [False],\n",
              "                         'loss': ['squared_hinge'], 'penalty': ['l1']},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring=None, verbose=0)"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kXwQhRxDwGUB",
        "outputId": "056f62c1-d7e5-4a8c-83b6-2e2d66441b95"
      },
      "source": [
        "clf_svm_h.best_params_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'C': 0.1, 'dual': False, 'loss': 'squared_hinge', 'penalty': 'l1'}"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3prGdbPwIBr",
        "outputId": "b744df85-8499-4415-b5ad-9dc0a4fb1967"
      },
      "source": [
        "f1_train = f1_score( list(train_df['label']),clf_svm_h.predict(binary_bgw_train), average='macro')\n",
        "f1_valid = f1_score( list(valid_df['label']),clf_svm_h.predict(binary_bgw_valid), average='macro')\n",
        "f1_test = f1_score( list(test_df['label']),clf_svm_h.predict(binary_bgw_test), average='macro')\n",
        "print(f'F1-score of Linear SVM with hyperparameter tunning\\n Training: {f1_train}\\n Validation: {f1_valid} \\n Test:{f1_test}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1-score of Linear SVM with hyperparameter tunning\n",
            " Training: 0.8828166816174052\n",
            " Validation: 0.8873905876846092 \n",
            " Test:0.8001591294732682\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XudEbY5nGztV"
      },
      "source": [
        "# 3.a Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCcBVH_YHDbn",
        "outputId": "4cf26b44-5c60-418a-c1f0-34db80e6f771"
      },
      "source": [
        "clf_gnb = GaussianNB()\n",
        "clf_gnb.fit(freq_bgw_train, train_df['label'])\n",
        "f1_train = f1_score( list(train_df['label']),clf_gnb.predict(freq_bgw_train), average='macro')\n",
        "f1_valid = f1_score( list(valid_df['label']),clf_gnb.predict(freq_bgw_valid), average='macro')\n",
        "f1_test = f1_score( list(test_df['label']),clf_gnb.predict(freq_bgw_test), average='macro')\n",
        "print(f'F1-score of  Bernouli Naive Bayes for frequency BoW without hyperparameter tunning\\n Training: {f1_train}\\n Vlidation: {f1_valid} \\n Test:{f1_test}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1-score of  Bernouli Naive Bayes for frequency BoW without hyperparameter tunning\n",
            " Training: 0.6924892955717259\n",
            " Vlidation: 0.3592475251688793 \n",
            " Test:0.35841623737894085\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1IwkI-N8HWvt",
        "outputId": "e82436c7-c2a9-4f28-9bee-7f62fdf67bf1"
      },
      "source": [
        "# clf = BernoulliNB()\n",
        "# clf.fit(train_df_total.iloc[:,:10000], train_df_total['label'])\n",
        "params = {'var_smoothing': np.logspace(-11, 3, num=50)}\n",
        "f = [-1 for i in range(train_df.shape[0])] + [1 for i in range(valid_df.shape[0])]\n",
        "p= PredefinedSplit(test_fold = f)\n",
        "clf_gnb_h = GridSearchCV(GaussianNB(), param_grid=params, cv=p, scoring='f1_macro')\n",
        "clf_gnb_h.fit(freq_bgw_total.iloc[:,:10000], freq_bgw_total['label'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=PredefinedSplit(test_fold=array([-1, -1, ...,  1,  1])),\n",
              "             error_score=nan,\n",
              "             estimator=GaussianNB(priors=None, var_smoothing=1e-09),\n",
              "             iid='deprecated', n_jobs=None,\n",
              "             param_grid={'var_smoothing': array([1.00000000e-11, 1.93069773e-11, 3.72759372e-11, 7.19685673e-11,\n",
              "       1.38949549e-10, 2.68269580e-10, 5.17947468e-10, 1.00000000e-09,\n",
              "       1.93069773e-09, 3.72759372e-09,...\n",
              "       1.38949549e-02, 2.68269580e-02, 5.17947468e-02, 1.00000000e-01,\n",
              "       1.93069773e-01, 3.72759372e-01, 7.19685673e-01, 1.38949549e+00,\n",
              "       2.68269580e+00, 5.17947468e+00, 1.00000000e+01, 1.93069773e+01,\n",
              "       3.72759372e+01, 7.19685673e+01, 1.38949549e+02, 2.68269580e+02,\n",
              "       5.17947468e+02, 1.00000000e+03])},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring='f1_macro', verbose=0)"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gMCaQ9nHHYs9",
        "outputId": "3daa2a03-d404-488c-ba5a-787cacf48594"
      },
      "source": [
        "clf_gnb_h.best_params_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'var_smoothing': 0.001}"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWYv-fBxHa1s",
        "outputId": "3859bceb-1ea2-4392-f8aa-312d46906778"
      },
      "source": [
        "f1_train = f1_score( list(train_df['label']),clf_gnb_h.predict(freq_bgw_train), average='macro')\n",
        "f1_valid = f1_score( list(valid_df['label']),clf_gnb_h.predict(freq_bgw_valid), average='macro')\n",
        "f1_test = f1_score( list(test_df['label']),clf_gnb_h.predict(freq_bgw_test), average='macro')\n",
        "print(f'F1-score of  Gaussian Naive Bayes for frequency BoW with smoothing var.={clf_gnb_h.best_params_}\\n Training: {f1_train}\\n Vlidation: {f1_valid} \\n Test:{f1_test}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1-score of  Gaussian Naive Bayes for frequency BoW with smoothing var.={'var_smoothing': 0.001}\n",
            " Training: 0.5769039084787\n",
            " Vlidation: 0.5449155234246417 \n",
            " Test:0.45004783517410624\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wi8t2WN8Kvxk"
      },
      "source": [
        "# 3.a Decision Trees"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ESDzpVfEK1yk",
        "outputId": "892874df-0b52-4301-de0a-07a79380aa02"
      },
      "source": [
        "clf_dec_f=DecisionTreeClassifier()\n",
        "clf_dec_f.fit(freq_bgw_train, train_df['label'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
              "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
              "                       random_state=None, splitter='best')"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzEzM_7tK738",
        "outputId": "5c205ad6-59a5-4a20-f775-212cf4093dac"
      },
      "source": [
        "f1_train = f1_score( list(train_df['label']),clf_dec_f.predict(freq_bgw_train), average='macro')\n",
        "f1_valid = f1_score( list(valid_df['label']),clf_dec_f.predict(freq_bgw_valid), average='macro')\n",
        "f1_test = f1_score( list(test_df['label']),clf_dec_f.predict(freq_bgw_test), average='macro')\n",
        "print(f'F1-score of Decision Tree for Frequency BoW without hyperparameter tunning\\n Training: {f1_train}\\n Validation: {f1_valid} \\n Test:{f1_test}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1-score of Decision Tree for Frequency BoW without hyperparameter tunning\n",
            " Training: 0.9084872405754333\n",
            " Validation: 0.7074749537410122 \n",
            " Test:0.7013769667229854\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VpsIZxrK_i0",
        "outputId": "2cd6ab6d-d574-4389-b3b5-e8f3e57b686e"
      },
      "source": [
        "# params = {'max_depth': np.arange(14, 19), 'max_features': np.arange(6000, 10001, 1000), 'max_leaf_nodes': np.arange(1500, 2500, 250), 'min_samples_leaf': np.arange(2, 6)}\n",
        "# params = {'ccp_alpha': np.arange(0, .005, .001),'max_depth': np.arange(14, 18), 'max_features': np.arange(7000, 10001, 1000), 'max_leaf_nodes': np.arange(1500, 2500, 250), 'min_samples_leaf': np.arange(1, 5)}\n",
        "params = {'ccp_alpha': np.arange(0, .005, .001)}\n",
        "f = [-1 for i in range(train_df.shape[0])] + [0 for i in range(valid_df.shape[0])]\n",
        "p= PredefinedSplit(test_fold = f)\n",
        "clf_dec_f_h = GridSearchCV(DecisionTreeClassifier(), param_grid=params, cv=p)\n",
        "clf_dec_f_h.fit(freq_bgw_total.iloc[:,:10000], freq_bgw_total['label'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=PredefinedSplit(test_fold=array([-1, -1, ...,  0,  0])),\n",
              "             error_score=nan,\n",
              "             estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,\n",
              "                                              criterion='gini', max_depth=None,\n",
              "                                              max_features=None,\n",
              "                                              max_leaf_nodes=None,\n",
              "                                              min_impurity_decrease=0.0,\n",
              "                                              min_impurity_split=None,\n",
              "                                              min_samples_leaf=1,\n",
              "                                              min_samples_split=2,\n",
              "                                              min_weight_fraction_leaf=0.0,\n",
              "                                              presort='deprecated',\n",
              "                                              random_state=None,\n",
              "                                              splitter='best'),\n",
              "             iid='deprecated', n_jobs=None,\n",
              "             param_grid={'ccp_alpha': array([0.   , 0.001, 0.002, 0.003, 0.004])},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring=None, verbose=0)"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fLFp6raLBYH",
        "outputId": "83e6e2ab-2a1d-4bb3-b391-34ed873098a2"
      },
      "source": [
        "clf_dec_f_h.best_params_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'ccp_alpha': 0.002}"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMepbTv0LDOf",
        "outputId": "b4149886-f4c0-4b30-8dfa-ac101f430570"
      },
      "source": [
        "f1_train = f1_score( list(train_df['label']),clf_dec_f_h.predict(freq_bgw_train), average='macro')\n",
        "f1_valid = f1_score( list(valid_df['label']),clf_dec_f_h.predict(freq_bgw_valid), average='macro')\n",
        "f1_test = f1_score( list(test_df['label']),clf_dec_f_h.predict(freq_bgw_test), average='macro')\n",
        "print(f'F1-score of Decision Tree for frequency BoW with hyperparameter tunning\\n Training: {f1_train}\\n Validation: {f1_valid} \\n Test:{f1_test}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1-score of Decision Tree for frequency BoW with hyperparameter tunning\n",
            " Training: 0.7733022429864261\n",
            " Validation: 0.7860427446769105 \n",
            " Test:0.7671615917065018\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_76OtzaLvMX"
      },
      "source": [
        "# 3.a Logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tg7Y2agyL6Qh"
      },
      "source": [
        "clf_lr_f = LogisticRegression().fit(freq_bgw_train, train_df['label'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5iTFbYOfMDuQ",
        "outputId": "e2e2cbe9-b143-4786-ae4a-af2e46a48d29"
      },
      "source": [
        "f1_train = f1_score( list(train_df['label']),clf_lr_f.predict(freq_bgw_train), average='macro')\n",
        "f1_valid = f1_score( list(valid_df['label']),clf_lr_f.predict(freq_bgw_valid), average='macro')\n",
        "f1_test = f1_score( list(test_df['label']),clf_lr_f.predict(freq_bgw_test), average='macro')\n",
        "print(f'F1-score of Logistic Regression for frequency BoW without hyperparameter tunning\\n Training: {f1_train}\\n Validation: {f1_valid} \\n Test:{f1_test}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1-score of Logistic Regression for frequency BoW without hyperparameter tunning\n",
            " Training: 0.3325629904424544\n",
            " Validation: 0.32519120594231016 \n",
            " Test:0.32947115540038147\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B642leavMKPw",
        "outputId": "41dec8f7-f242-4726-a9c8-1b03c1bcef94"
      },
      "source": [
        "import numpy as np\n",
        "params = {'penalty':['l1','l2'], 'C':np.arange(40, 100, 5), 'solver':['liblinear', 'saga']}\n",
        "# params = {'penalty':['l1'], 'solver':['liblinear'], 'C':np.arange(.2, .4, .1)}\n",
        "f = [-1 for i in range(train_df.shape[0])] + [1 for i in range(valid_df.shape[0])]\n",
        "p= PredefinedSplit(test_fold = f)\n",
        "clf_lr_f_h = GridSearchCV(LogisticRegression(), param_grid=params, cv=p, scoring='f1_macro')\n",
        "clf_lr_f_h.fit(freq_bgw_total.iloc[:,:10000], freq_bgw_total['label'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=PredefinedSplit(test_fold=array([-1, -1, ...,  1,  1])),\n",
              "             error_score=nan,\n",
              "             estimator=LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
              "                                          fit_intercept=True,\n",
              "                                          intercept_scaling=1, l1_ratio=None,\n",
              "                                          max_iter=100, multi_class='auto',\n",
              "                                          n_jobs=None, penalty='l2',\n",
              "                                          random_state=None, solver='lbfgs',\n",
              "                                          tol=0.0001, verbose=0,\n",
              "                                          warm_start=False),\n",
              "             iid='deprecated', n_jobs=None,\n",
              "             param_grid={'C': array([40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95]),\n",
              "                         'penalty': ['l1', 'l2'],\n",
              "                         'solver': ['liblinear', 'saga']},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring='f1_macro', verbose=0)"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iLbHIVjtCmH8",
        "outputId": "c65c99eb-cf31-44e1-a53d-4cdf3436d394"
      },
      "source": [
        "clf_lr_f_h.best_params_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'C': 75, 'penalty': 'l1', 'solver': 'liblinear'}"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YqjFrYS7Me_Z",
        "outputId": "3765400f-90cd-4d0d-d89e-d816d3a14708"
      },
      "source": [
        "f1_train = f1_score( list(train_df['label']),clf_lr_f_h.predict(freq_bgw_train), average='macro')\n",
        "f1_valid = f1_score( list(valid_df['label']),clf_lr_f_h.predict(freq_bgw_valid), average='macro')\n",
        "f1_test = f1_score( list(test_df['label']),clf_lr_f_h.predict(freq_bgw_test), average='macro')\n",
        "print(f'F1-score of Logistic Regression for frequency BoW with hyperparameter tunning\\n Training: {f1_train}\\n Validation: {f1_valid} \\n Test:{f1_test}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1-score of Logistic Regression for frequency BoW with hyperparameter tunning\n",
            " Training: 0.8574325884731568\n",
            " Validation: 0.8610039148141776 \n",
            " Test:0.770223539335674\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEvPrtM5M4k7"
      },
      "source": [
        "# 3.a Linear SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXC9MXVyNAh7"
      },
      "source": [
        "clf_svm_f = LinearSVC().fit(freq_bgw_train, train_df['label'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aKLI7SmuNFRK",
        "outputId": "4d3f1509-b235-4134-f9ed-f0d16f7455b6"
      },
      "source": [
        "f1_train = f1_score( list(train_df['label']),clf_svm_f.predict(freq_bgw_train), average='macro')\n",
        "f1_valid = f1_score( list(valid_df['label']),clf_svm_f.predict(freq_bgw_valid), average='macro')\n",
        "f1_test = f1_score( list(test_df['label']),clf_svm_f.predict(freq_bgw_test), average='macro')\n",
        "print(f'F1-score of Linear SVM for frequency BoW without hyperparameter tunning\\n Training: {f1_train}\\n Validation: {f1_valid} \\n Test:{f1_test}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1-score of Linear SVM for frequency BoW without hyperparameter tunning\n",
            " Training: 0.4017287747626928\n",
            " Validation: 0.38635388240794466 \n",
            " Test:0.36746611986152194\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z488AGZ4NJTH",
        "outputId": "77763ca6-e4be-4bd4-d856-67b21aa820e4"
      },
      "source": [
        "params = {'penalty':['l1','l2'], 'C':np.arange(200, 400, 10), 'loss':['hinge', 'squared_hinge'], 'dual':[True,False]}\n",
        "# params = {'penalty':['l1'], 'C':np.arange(1, 100, 20), 'loss':['squared_hinge'], 'dual':['False'], 'random_state':[0]}\n",
        "f = [-1 for i in range(train_df.shape[0])] + [1 for i in range(valid_df.shape[0])]\n",
        "p= PredefinedSplit(test_fold = f)\n",
        "clf_svm_f_h = GridSearchCV(LinearSVC(), param_grid=params, cv=p)\n",
        "clf_svm_f_h.fit(freq_bgw_total.iloc[:,:10000], freq_bgw_total['label'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=PredefinedSplit(test_fold=array([-1, -1, ...,  1,  1])),\n",
              "             error_score=nan,\n",
              "             estimator=LinearSVC(C=1.0, class_weight=None, dual=True,\n",
              "                                 fit_intercept=True, intercept_scaling=1,\n",
              "                                 loss='squared_hinge', max_iter=1000,\n",
              "                                 multi_class='ovr', penalty='l2',\n",
              "                                 random_state=None, tol=0.0001, verbose=0),\n",
              "             iid='deprecated', n_jobs=None,\n",
              "             param_grid={'C': array([200, 210, 220, 230, 240, 250, 260, 270, 280, 290, 300, 310, 320,\n",
              "       330, 340, 350, 360, 370, 380, 390]),\n",
              "                         'dual': [True, False],\n",
              "                         'loss': ['hinge', 'squared_hinge'],\n",
              "                         'penalty': ['l1', 'l2']},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring=None, verbose=0)"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IhjMfNzINLkV",
        "outputId": "2cd95cc3-b2ce-48c7-8365-38e38639da3b"
      },
      "source": [
        "clf_svm_f_h.best_params_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'C': 240, 'dual': False, 'loss': 'squared_hinge', 'penalty': 'l1'}"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bNRHV9-NNU7",
        "outputId": "8529f0e8-fb5a-4d6b-ae39-6ff2f9f1e26e"
      },
      "source": [
        "f1_train = f1_score( list(train_df['label']),clf_svm_f_h.predict(freq_bgw_train), average='macro')\n",
        "f1_valid = f1_score( list(valid_df['label']),clf_svm_f_h.predict(freq_bgw_valid), average='macro')\n",
        "f1_test = f1_score( list(test_df['label']),clf_svm_f_h.predict(freq_bgw_test), average='macro')\n",
        "print(f'F1-score of Linear SVM for frequency BoW with hyperparameter tunning\\n Training: {f1_train}\\n Validation: {f1_valid} \\n Test:{f1_test}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1-score of Linear SVM for frequency BoW with hyperparameter tunning\n",
            " Training: 0.8979681728512134\n",
            " Validation: 0.9072911625334804 \n",
            " Test:0.7583647448389872\n"
          ]
        }
      ]
    }
  ]
}